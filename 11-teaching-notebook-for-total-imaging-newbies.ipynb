{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pathlib\nimport imageio\nimport numpy as np\n\n# Glob the training data and load a single image path\ntraining_paths = pathlib.Path('../input/stage1_train').glob('*/images/*.png')\ntraining_sorted = sorted([x for x in training_paths])\nim_path = training_sorted[45]\nim = imageio.imread(str(im_path))","metadata":{"_cell_guid":"2b168289-7746-4255-b933-356bd04914b7","_uuid":"25fa830192ea11807891ec2f71487dbeb18364e3","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# Print the image dimensions\nprint('Original image shape: {}'.format(im.shape))\n\n# Coerce the image into grayscale format (if not already)\nfrom skimage.color import rgb2gray\nim_gray = rgb2gray(im)\nprint('New image shape: {}'.format(im_gray.shape))","metadata":{"_cell_guid":"3f389854-b251-4723-babf-9fd0d71e3805","_uuid":"3f03d9fd9cb23494b936e2ebd4535878efc2ccfd","trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# Now, let's plot the data\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,4))\n\nplt.subplot(1,2,1)\nplt.imshow(im)\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1,2,2)\nplt.imshow(im_gray, cmap='gray')\nplt.axis('off')\nplt.title('Grayscale Image')\n\nplt.tight_layout()\nplt.show()","metadata":{"_cell_guid":"c298bcd0-2c4b-42aa-b0d1-f5b829515841","_kg_hide-input":true,"_uuid":"b1e6a69a5ae9121ff4f2f1a4fa96698f532a9375","trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"from skimage.filters import threshold_otsu\nthresh_val = threshold_otsu(im_gray)\nmask = np.where(im_gray > thresh_val, 1, 0)\n\n# Make sure the larger portion of the mask is considered background\nif np.sum(mask==0) < np.sum(mask==1):\n    mask = np.where(mask, 0, 1)","metadata":{"_cell_guid":"f145ff54-8e4d-4546-9913-e695bb7d9254","_uuid":"b76b12a4b26dc1a351d717a047bd90ea6aa3f7dd","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\n\nplt.subplot(1,2,1)\nim_pixels = im_gray.flatten()\nplt.hist(im_pixels,bins=50)\nplt.vlines(thresh_val, 0, 100000, linestyle='--')\nplt.ylim([0,50000])\nplt.title('Grayscale Histogram')\n\nplt.subplot(1,2,2)\nmask_for_display = np.where(mask, mask, np.nan)\nplt.imshow(im_gray, cmap='gray')\nplt.imshow(mask_for_display, cmap='rainbow', alpha=0.5)\nplt.axis('off')\nplt.title('Image w/ Mask')\n\nplt.show()","metadata":{"_cell_guid":"65033fba-1681-46fd-aa6d-c3e3fec09c1f","_kg_hide-input":true,"_uuid":"14792479e2019e59f6cd0388b40798debb9e34dc","trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"from scipy import ndimage\nlabels, nlabels = ndimage.label(mask)\n\nlabel_arrays = []\nfor label_num in range(1, nlabels+1):\n    label_mask = np.where(labels == label_num, 1, 0)\n    label_arrays.append(label_mask)\n\nprint('There are {} separate components / objects detected.'.format(nlabels))","metadata":{"_cell_guid":"5c8b868f-1b2d-4939-914f-836d0aa7b50d","_uuid":"2475893dbeabff7c165283970eebea2fd2de9bd7","trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# Create a random colormap\nfrom matplotlib.colors import ListedColormap\nrand_cmap = ListedColormap(np.random.rand(256,3))\n\nlabels_for_display = np.where(labels > 0, labels, np.nan)\nplt.imshow(im_gray, cmap='gray')\nplt.imshow(labels_for_display, cmap=rand_cmap)\nplt.axis('off')\nplt.title('Labeled Cells ({} Nuclei)'.format(nlabels))\nplt.show()","metadata":{"_cell_guid":"14e0c916-d8c9-42df-acd0-3a89f4d0d072","_kg_hide-input":true,"_uuid":"53cb03a6457b538ad803ffe4fc1159b085f2a72e","trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"for label_ind, label_coords in enumerate(ndimage.find_objects(labels)):\n    cell = im_gray[label_coords]\n    \n    # Check if the label size is too small\n    if np.product(cell.shape) < 10: \n        print('Label {} is too small! Setting to 0.'.format(label_ind))\n        mask = np.where(labels==label_ind+1, 0, mask)\n\n# Regenerate the labels\nlabels, nlabels = ndimage.label(mask)\nprint('There are now {} separate components / objects detected.'.format(nlabels))","metadata":{"_uuid":"34ab91ec5564752381fe300645aa4e4732288e14","trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,6, figsize=(10,6))\n\nfor ii, obj_indices in enumerate(ndimage.find_objects(labels)[0:6]):\n    cell = im_gray[obj_indices]\n    axes[ii].imshow(cell, cmap='gray')\n    axes[ii].axis('off')\n    axes[ii].set_title('Label #{}\\nSize: {}'.format(ii+1, cell.shape))\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"_uuid":"28cc9fda0ca6b5e0060591ca9ed7f41d52919a20","trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"Label #2 has the \"adjacent cell\" problem: the two cells are being considered part of the same object. One thing we can do here is to see whether we can shrink the mask to \"open up\" the differences between the cells. This is called mask erosion. We can then re-dilate it to to recover the original proportions. ","metadata":{"_uuid":"c44a9f56a96eb0b65464f1b20a3738dc4051826d"}},{"cell_type":"code","source":"# Get the object indices, and perform a binary opening procedure\ntwo_cell_indices = ndimage.find_objects(labels)[1]\ncell_mask = mask[two_cell_indices]\ncell_mask_opened = ndimage.binary_opening(cell_mask, iterations=8)","metadata":{"_uuid":"aea9549db2f19d7551fb2a021bd877572c8555fb","trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,4, figsize=(12,4))\n\naxes[0].imshow(im_gray[two_cell_indices], cmap='gray')\naxes[0].set_title('Original object')\naxes[1].imshow(mask[two_cell_indices], cmap='gray')\naxes[1].set_title('Original mask')\naxes[2].imshow(cell_mask_opened, cmap='gray')\naxes[2].set_title('Opened mask')\naxes[3].imshow(im_gray[two_cell_indices]*cell_mask_opened, cmap='gray')\naxes[3].set_title('Opened object')\n\n\nfor ax in axes:\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"_uuid":"cc2b1241e08673952bc6da9f4d05b95608a2c1e8","trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"def rle_encoding(x):\n    '''\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    '''\n    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b+1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return \" \".join([str(i) for i in run_lengths])\n\nprint('RLE Encoding for the current mask is: {}'.format(rle_encoding(label_mask)))","metadata":{"_cell_guid":"29ea6b76-766c-487d-8f88-d6b0e288e94f","_uuid":"4d64b21aa0f26d177863b3cae7fb91e80bc39180","trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef analyze_image(im_path):\n    '''\n    Take an image_path (pathlib.Path object), preprocess and label it, extract the RLE strings \n    and dump it into a Pandas DataFrame.\n    '''\n    # Read in data and convert to grayscale\n    im_id = im_path.parts[-3]\n    im = imageio.imread(str(im_path))\n    im_gray = rgb2gray(im)\n    \n    # Mask out background and extract connected objects\n    thresh_val = threshold_otsu(im_gray)\n    mask = np.where(im_gray > thresh_val, 1, 0)\n    if np.sum(mask==0) < np.sum(mask==1):\n        mask = np.where(mask, 0, 1)    \n        labels, nlabels = ndimage.label(mask)\n    labels, nlabels = ndimage.label(mask)\n    \n    # Loop through labels and add each to a DataFrame\n    im_df = pd.DataFrame()\n    for label_num in range(1, nlabels+1):\n        label_mask = np.where(labels == label_num, 1, 0)\n        if label_mask.flatten().sum() > 10:\n            rle = rle_encoding(label_mask)\n            s = pd.Series({'ImageId': im_id, 'EncodedPixels': rle})\n            im_df = im_df.append(s, ignore_index=True)\n    \n    return im_df\n\n\ndef analyze_list_of_images(im_path_list):\n    '''\n    Takes a list of image paths (pathlib.Path objects), analyzes each,\n    and returns a submission-ready DataFrame.'''\n    all_df = pd.DataFrame()\n    for im_path in im_path_list:\n        im_df = analyze_image(im_path)\n        all_df = all_df.append(im_df, ignore_index=True)\n    \n    return all_df","metadata":{"_cell_guid":"0d32b900-8f40-4eff-8465-0f715f24faf6","_uuid":"944f216004fa635201470b8eaa28a446d9679516","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"testing = pathlib.Path('../input/stage1_test/').glob('*/images/*.png')\ndf = analyze_list_of_images(list(testing))\ndf.to_csv('submission.csv', index=None)","metadata":{"_cell_guid":"58b07c91-855f-4e6d-a9d8-457750d9ad4a","_uuid":"b7e21d3b95d7936904bfed088a880a5c78f0fa18","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}